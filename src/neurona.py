# -*- coding: utf-8 -*-
"""Neurona.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g8RG7-9808Vn8t7EtlEp89A-zfX_Ubzf
"""

import numpy as np

class Neuron:
    def __init__(self, weights, bias, func="relu"):
        self.weights = weights
        self.bias = bias
        self.func = func

    def changeBias(self, b):
        self.bias = b

    def changeWeights(self, weights):
        self.weights = weights

    def run(self, input_data):
        weighted_sum = sum(w * x for w, x in zip(self.weights, input_data))
        output = self.__activate(weighted_sum + self.bias)
        return output

    @staticmethod
    def __relu(x):
        return max(x, 0)

    @staticmethod
    def __sigmoid(x):
        return 1 / (1 + np.exp(-x))

    @staticmethod
    def __tanh(x):
        return np.tanh(x)

    def __activate(self, x):
        activation_functions = {
            "relu": self.__relu,
            "sigmoid": self.__sigmoid,
            "tanh": self.__tanh,
        }

        activation_function = activation_functions.get(self.func)
        if activation_function is not None:
            return activation_function(x)
        else:
            raise ValueError("Invalid activation function")


n1 = Neuron(weights=[0.5, 1.2, 6.8], bias=100, func="relu")
x = [4, 6, -3]
output = n1.run(input_data=x)
print(output)

n1.changeBias(-100)
output = n1.run(input_data=x)
print(output)


n2 = Neuron(weights=[0.5, 1.2, 6.8], bias=100, func="tanh")
output_tanh = n2.run(input_data=x)
print(output_tanh)